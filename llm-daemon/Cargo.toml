[package]
name = "llm-daemon"
version.workspace = true
authors.workspace = true
edition = "2021"
license = "GPL-3.0-only"
keywords = ["llm"]
description = "LLM as a daemon"
readme = "README.md"
repository = "https://github.com/blmarket/llm-daemon"

[dependencies]
llama_cpp_low = { version = "0.7.0", path = "../llama-cpp-low", default-features = false }
tracing = { workspace = true }
anyhow = { workspace = true }
tokio = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
axum = "0.8"
daemonize = "0.5"
futures = "0.3"
hyper = { version = "1", features = ["full"] }
hyper-util = { version = "0.1", features = ["full"] }
reqwest = { version = "0.12", default-features = false, features = ["json", "rustls-tls"] }
tempfile = "3"
url = "2"
tracing-subscriber = { version = "0.3", features = ["local-time"] }

[dev-dependencies]
tracing-test = "0.2"
criterion = "0.5"
langchain-rust = "4"
async-trait = "0.1"

[features]
default = ["llama-daemon", "proxy"]
llama-daemon = []
cuda = ["llama_cpp_low/cuda"]
proxy = []

[[bench]]
name = "port_open"
harness = false    

[[example]]
name = "langchain_llama"
required-features = ["llama-daemon"]


[[example]]
name = "with_llama_curl"
required-features = ["llama-daemon"]

[[example]]
name = "simple"
required-features = ["llama-daemon"]

[[example]]
name = "qwen3-coder"
required-features = ["llama-daemon", "cuda"]

[[example]]
name = "gpt-oss-20b"
required-features = ["llama-daemon", "cuda"]
